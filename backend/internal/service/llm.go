package service

import (
	"aiwisper/session"
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"
)

type LLMService struct{}

func NewLLMService() *LLMService {
	return &LLMService{}
}

// GenerateSummaryWithLLM generates a summary using Ollama or fallback
func (s *LLMService) GenerateSummaryWithLLM(transcriptText string, ollamaModel string, ollamaUrl string) (string, error) {
	summary, err := s.generateSummaryWithOllama(transcriptText, ollamaModel, ollamaUrl)
	if err == nil && summary != "" {
		return summary, nil
	}
	log.Printf("Ollama not available: %v, using fallback...", err)
	return s.generateSummaryFallback(transcriptText)
}

func (s *LLMService) generateSummaryWithOllama(transcriptText string, model string, baseUrl string) (string, error) {
	resp, err := http.Get(baseUrl + "/api/tags")
	if err != nil {
		return "", fmt.Errorf("Ollama not running at %s", baseUrl)
	}
	resp.Body.Close()

	maxChars := 16000
	text := transcriptText
	if len(text) > maxChars {
		text = text[:maxChars] + "\n...[text trimmed]..."
	}

	systemPrompt := `–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–∞—Ç–∫–∏—Ö —Ä–µ–∑—é–º–µ –¥–µ–ª–æ–≤—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –∏ –≤—Å—Ç—Ä–µ—á.
–¢–í–û–Ø –ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ.
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ –≤ Markdown):
## üìã –¢–µ–º–∞ –≤—Å—Ç—Ä–µ—á–∏
[1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è]
## üéØ –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
- [–ø—É–Ω–∫—Ç 1]
## ‚úÖ –†–µ—à–µ–Ω–∏—è –∏ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏
- [–ø—É–Ω–∫—Ç 1]
## üìå –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
- [–ø—É–Ω–∫—Ç 1]
–ü–†–ê–í–ò–õ–ê: Markdown, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.`

	userPrompt := fmt.Sprintf("–í–æ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:\n\n%s", text)

	reqBody := map[string]interface{}{
		"model": model,
		"messages": []map[string]string{
			{"role": "system", "content": systemPrompt},
			{"role": "user", "content": userPrompt},
		},
		"stream": false,
		"options": map[string]interface{}{
			"temperature": 0.3,
			"num_predict": 4096,
		},
	}

	return s.callOllama(baseUrl, reqBody)
}

func (s *LLMService) generateSummaryFallback(transcriptText string) (string, error) {
	lines := strings.Split(transcriptText, "\n")
	if len(lines) == 0 {
		return "", fmt.Errorf("empty transcript")
	}

	var youLines, otherLines, totalWords int
	for _, line := range lines {
		line = strings.TrimSpace(line)
		if line == "" {
			continue
		}
		words := strings.Fields(line)
		totalWords += len(words)
		if strings.HasPrefix(line, "–í—ã:") {
			youLines++
		} else if strings.HasPrefix(line, "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫:") {
			otherLines++
		}
	}

	summary := fmt.Sprintf(`üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–ø–∏—Å–∏:
‚Ä¢ –†–µ–ø–ª–∏–∫ "–í—ã": %d
‚Ä¢ –†–µ–ø–ª–∏–∫ "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫": %d  
‚Ä¢ –í—Å–µ–≥–æ —Å–ª–æ–≤: %d
üí° –î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ AI-–∞–Ω–∞–ª–∏–∑–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama.`, youLines, otherLines, totalWords)
	return summary, nil
}

// ImproveTranscriptionWithLLM improves transcription quality
func (s *LLMService) ImproveTranscriptionWithLLM(dialogue []session.TranscriptSegment, ollamaModel string, ollamaUrl string) ([]session.TranscriptSegment, error) {
	resp, err := http.Get(ollamaUrl + "/api/tags")
	if err != nil {
		return nil, fmt.Errorf("Ollama not running at %s", ollamaUrl)
	}
	resp.Body.Close()

	var dialogueText strings.Builder
	for _, seg := range dialogue {
		speaker := "–í—ã"
		if seg.Speaker != "" && seg.Speaker != "mic" {
			// –ü–æ–¥–¥–µ—Ä–∂–∫–∞ "sys", "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫", "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫ 1", "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫ 2" –∏ —Ç.–¥.
			if strings.HasPrefix(seg.Speaker, "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫") {
				speaker = seg.Speaker
			} else {
				speaker = "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫"
			}
		}
		dialogueText.WriteString(fmt.Sprintf("[%s] %s\n", speaker, seg.Text))
	}

	text := dialogueText.String()
	if len(text) > 12000 {
		text = text[:12000] + "\n...[trimmed]..."
	}

	systemPrompt := `–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏.

–¢–í–û–ò –ó–ê–î–ê–ß–ò (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞):
1. –†–ê–ó–î–ï–õ–Ø–ô –°–ö–õ–ï–ï–ù–ù–´–ï –°–õ–û–í–ê: "–≤–æ–ø—Ä–æ—Å–µ—è–Ω–µ–º–æ–∂–æ" ‚Üí "–≤–æ–ø—Ä–æ—Å–µ —è –Ω–µ –º–æ–≥—É", "–∫–∞–∫–æ–º—Å–æ—Å—Ç–æ—è–Ω–∏" ‚Üí "–∫–∞–∫–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏"
2. –î–æ–±–∞–≤–ª—è–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é: —Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ, –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏
3. –ò—Å–ø—Ä–∞–≤–ª—è–π —Ä–µ–≥–∏—Å—Ç—Ä: –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
4. –ò—Å–ø—Ä–∞–≤–ª—è–π –æ—á–µ–≤–∏–¥–Ω—ã–µ –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (–æ–ø–µ—á–∞—Ç–∫–∏, –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –±—É–∫–≤—ã)
5. –†–ê–ó–ë–ò–í–ê–ô –¥–ª–∏–Ω–Ω—ã–µ —Ä–µ–ø–ª–∏–∫–∏ (–±–æ–ª—å—à–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å —Ç–µ–º –∂–µ —Å–ø–∏–∫–µ—Ä–æ–º

–§–û–†–ú–ê–¢ –í–•–û–î–ê:
[–í—ã] —Ç–µ–∫—Å—Ç —Ä–µ–ø–ª–∏–∫–∏
[–°–æ–±–µ—Å–µ–¥–Ω–∏–∫] —Ç–µ–∫—Å—Ç —Ä–µ–ø–ª–∏–∫–∏

–§–û–†–ú–ê–¢ –í–´–•–û–î–ê (—Å—Ç—Ä–æ–≥–æ —Ç–∞–∫–æ–π –∂–µ):
[–í—ã] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
[–°–æ–±–µ—Å–µ–¥–Ω–∏–∫] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.

–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
- –ù–ï –º–µ–Ω—è–π —Å–º—ã—Å–ª –∏ –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤
- –ù–ï —É–¥–∞–ª—è–π –∏ –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ä–µ–ø–ª–∏–∫–∏
- –ù–ï –æ–±—ä–µ–¥–∏–Ω—è–π —Ä–µ–ø–ª–∏–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
- –°–æ—Ö—Ä–∞–Ω—è–π –ø–æ—Ä—è–¥–æ–∫ —Ä–µ–ø–ª–∏–∫
- –ï—Å–ª–∏ —Ä–µ–ø–ª–∏–∫–∞ –¥–ª–∏–Ω–Ω–∞—è ‚Äî —Ä–∞–∑–±–µ–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ —Å –¢–ï–ú –ñ–ï —Å–ø–∏–∫–µ—Ä–æ–º
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤`

	userPrompt := fmt.Sprintf("–£–ª—É—á—à–∏ —ç—Ç—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é:\n\n%s", text)

	reqBody := map[string]interface{}{
		"model": ollamaModel,
		"messages": []map[string]string{
			{"role": "system", "content": systemPrompt},
			{"role": "user", "content": userPrompt},
		},
		"stream":  false,
		"options": map[string]interface{}{"temperature": 0.1, "num_predict": 8192},
	}

	response, err := s.callOllama(ollamaUrl, reqBody)
	if err != nil {
		return nil, err
	}

	return s.parseImprovedDialogue(response, dialogue), nil
}

func (s *LLMService) parseImprovedDialogue(improvedText string, originalDialogue []session.TranscriptSegment) []session.TranscriptSegment {
	lines := strings.Split(improvedText, "\n")
	var improved []session.TranscriptSegment
	origIdx := 0 // –ò–Ω–¥–µ–∫—Å –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ –¥–ª—è timestamps
	var lastSpeaker string

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if line == "" {
			continue
		}

		var speaker, text string

		// –ü–∞—Ä—Å–∏–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Å–ø–∏–∫–µ—Ä–æ–≤
		switch {
		case strings.HasPrefix(line, "[–í—ã]"):
			speaker = "mic"
			text = strings.TrimPrefix(line, "[–í—ã]")
		case strings.HasPrefix(line, "[–°–æ–±–µ—Å–µ–¥–Ω–∏–∫"):
			// –ü–æ–¥–¥–µ—Ä–∂–∫–∞ [–°–æ–±–µ—Å–µ–¥–Ω–∏–∫], [–°–æ–±–µ—Å–µ–¥–Ω–∏–∫ 1], [–°–æ–±–µ—Å–µ–¥–Ω–∏–∫ 2] –∏ —Ç.–¥.
			speaker = "sys"
			idx := strings.Index(line, "]")
			if idx > 0 {
				text = line[idx+1:]
			}
		case strings.HasPrefix(line, "–í—ã:"):
			speaker = "mic"
			text = strings.TrimPrefix(line, "–í—ã:")
		case strings.HasPrefix(line, "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫"):
			// –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –°–æ–±–µ—Å–µ–¥–Ω–∏–∫:, –°–æ–±–µ—Å–µ–¥–Ω–∏–∫ 1:, –°–æ–±–µ—Å–µ–¥–Ω–∏–∫ 2: –∏ —Ç.–¥.
			speaker = "sys"
			idx := strings.Index(line, ":")
			if idx > 0 {
				text = line[idx+1:]
			}
		default:
			// –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ - —ç—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ä–µ–ø–ª–∏–∫–∏
			// –∏–ª–∏ –º—É—Å–æ—Ä –æ—Ç LLM - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
			continue
		}

		text = strings.TrimSpace(text)
		if text == "" {
			continue
		}

		// –û–ø—Ä–µ–¥–µ–ª—è–µ–º timestamps
		var start, end int64

		// –ï—Å–ª–∏ —Å–ø–∏–∫–µ—Ä —Å–º–µ–Ω–∏–ª—Å—è - –±–µ—Ä—ë–º —Å–ª–µ–¥—É—é—â–∏–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
		// –ï—Å–ª–∏ —Ç–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä (—Ä–∞–∑–±–∏—Ç–∞—è —Ä–µ–ø–ª–∏–∫–∞) - –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º –≤—Ä–µ–º—è
		if speaker != lastSpeaker {
			// –ù–æ–≤—ã–π —Å–ø–∏–∫–µ—Ä - —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º
			if origIdx < len(originalDialogue) {
				start = originalDialogue[origIdx].Start
				end = originalDialogue[origIdx].End
				origIdx++
			}
		} else {
			// –¢–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä - —ç—Ç–æ —Ä–∞–∑–±–∏—Ç–∞—è —Ä–µ–ø–ª–∏–∫–∞ –æ—Ç LLM
			// –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ)
			if len(improved) > 0 {
				prev := improved[len(improved)-1]
				start = prev.End
				end = start + 2000 // +2 —Å–µ–∫—É–Ω–¥—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
				// –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç —Å —Ç–µ–º –∂–µ —Å–ø–∏–∫–µ—Ä–æ–º - –ø–æ–¥—Ç—è–≥–∏–≤–∞–µ–º
				if origIdx < len(originalDialogue) && originalDialogue[origIdx].Speaker == speaker {
					end = originalDialogue[origIdx].End
					origIdx++
				}
			}
		}

		lastSpeaker = speaker

		improved = append(improved, session.TranscriptSegment{
			Start: start, End: end, Text: text, Speaker: speaker,
		})
	}

	if len(improved) == 0 {
		return originalDialogue
	}
	return improved
}

func (s *LLMService) callOllama(baseUrl string, reqBody interface{}) (string, error) {
	jsonBody, _ := json.Marshal(reqBody)
	client := &http.Client{Timeout: 300 * time.Second}
	resp, err := client.Post(baseUrl+"/api/chat", "application/json", bytes.NewBuffer(jsonBody))
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	bodyBytes, _ := io.ReadAll(resp.Body)
	var result struct {
		Message struct {
			Content string `json:"content"`
		} `json:"message"`
		Error string `json:"error"`
	}
	json.Unmarshal(bodyBytes, &result)

	if result.Error != "" {
		return "", fmt.Errorf("Ollama error: %s", result.Error)
	}
	return strings.TrimSpace(result.Message.Content), nil
}

// OllamaModel represents a model from Ollama API
type OllamaModel struct {
	Name       string `json:"name"`
	Size       int64  `json:"size"`
	ModifiedAt string `json:"modified_at"`
	Digest     string `json:"digest"`
	Details    struct {
		Format            string   `json:"format"`
		Family            string   `json:"family"`
		Families          []string `json:"families"`
		ParameterSize     string   `json:"parameter_size"`
		QuantizationLevel string   `json:"quantization_level"`
	} `json:"details"`
}

// GetOllamaModels gets models list from Ollama
func (s *LLMService) GetOllamaModels(baseUrl string) ([]OllamaModel, error) {
	client := &http.Client{Timeout: 5 * time.Second}
	resp, err := client.Get(baseUrl + "/api/tags")
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("Ollama API returned status: %d", resp.StatusCode)
	}

	var result struct {
		Models []OllamaModel `json:"models"`
	}

	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	if err := json.Unmarshal(bodyBytes, &result); err != nil {
		return nil, err
	}

	return result.Models, nil
}
