package service

import (
	"aiwisper/session"
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"
)

type LLMService struct{}

func NewLLMService() *LLMService {
	return &LLMService{}
}

// GenerateSummaryWithLLM generates a summary using Ollama or fallback
func (s *LLMService) GenerateSummaryWithLLM(transcriptText string, ollamaModel string, ollamaUrl string) (string, error) {
	summary, err := s.generateSummaryWithOllama(transcriptText, ollamaModel, ollamaUrl)
	if err == nil && summary != "" {
		return summary, nil
	}
	log.Printf("Ollama not available: %v, using fallback...", err)
	return s.generateSummaryFallback(transcriptText)
}

func (s *LLMService) generateSummaryWithOllama(transcriptText string, model string, baseUrl string) (string, error) {
	resp, err := http.Get(baseUrl + "/api/tags")
	if err != nil {
		return "", fmt.Errorf("Ollama not running at %s", baseUrl)
	}
	resp.Body.Close()

	maxChars := 16000
	text := transcriptText
	if len(text) > maxChars {
		text = text[:maxChars] + "\n...[text trimmed]..."
	}

	systemPrompt := `–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–∞—Ç–∫–∏—Ö —Ä–µ–∑—é–º–µ –¥–µ–ª–æ–≤—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –∏ –≤—Å—Ç—Ä–µ—á.
–¢–í–û–Ø –ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ.
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ –≤ Markdown):
## üìã –¢–µ–º–∞ –≤—Å—Ç—Ä–µ—á–∏
[1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è]
## üéØ –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
- [–ø—É–Ω–∫—Ç 1]
## ‚úÖ –†–µ—à–µ–Ω–∏—è –∏ –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏
- [–ø—É–Ω–∫—Ç 1]
## üìå –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
- [–ø—É–Ω–∫—Ç 1]
–ü–†–ê–í–ò–õ–ê: Markdown, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.`

	userPrompt := fmt.Sprintf("–í–æ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:\n\n%s", text)

	reqBody := map[string]interface{}{
		"model": model,
		"messages": []map[string]string{
			{"role": "system", "content": systemPrompt},
			{"role": "user", "content": userPrompt},
		},
		"stream": false,
		"options": map[string]interface{}{
			"temperature": 0.3,
			"num_predict": 4096,
		},
	}

	return s.callOllama(baseUrl, reqBody)
}

func (s *LLMService) generateSummaryFallback(transcriptText string) (string, error) {
	lines := strings.Split(transcriptText, "\n")
	if len(lines) == 0 {
		return "", fmt.Errorf("empty transcript")
	}

	var youLines, otherLines, totalWords int
	for _, line := range lines {
		line = strings.TrimSpace(line)
		if line == "" {
			continue
		}
		words := strings.Fields(line)
		totalWords += len(words)
		if strings.HasPrefix(line, "–í—ã:") {
			youLines++
		} else if strings.HasPrefix(line, "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫:") {
			otherLines++
		}
	}

	summary := fmt.Sprintf(`üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–ø–∏—Å–∏:
‚Ä¢ –†–µ–ø–ª–∏–∫ "–í—ã": %d
‚Ä¢ –†–µ–ø–ª–∏–∫ "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫": %d  
‚Ä¢ –í—Å–µ–≥–æ —Å–ª–æ–≤: %d
üí° –î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ AI-–∞–Ω–∞–ª–∏–∑–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama.`, youLines, otherLines, totalWords)
	return summary, nil
}

// ImproveTranscriptionWithLLM improves transcription quality
func (s *LLMService) ImproveTranscriptionWithLLM(dialogue []session.TranscriptSegment, ollamaModel string, ollamaUrl string) ([]session.TranscriptSegment, error) {
	resp, err := http.Get(ollamaUrl + "/api/tags")
	if err != nil {
		return nil, fmt.Errorf("Ollama not running at %s", ollamaUrl)
	}
	resp.Body.Close()

	var dialogueText strings.Builder
	for _, seg := range dialogue {
		speaker := "–í—ã"
		if seg.Speaker == "sys" {
			speaker = "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫"
		}
		dialogueText.WriteString(fmt.Sprintf("[%s] %s\n", speaker, seg.Text))
	}

	text := dialogueText.String()
	if len(text) > 12000 {
		text = text[:12000] + "\n...[trimmed]..."
	}

	systemPrompt := `–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π —Ä–µ—á–∏.
–ò—Å–ø—Ä–∞–≤–ª—è–π –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ä–µ–≥–∏—Å—Ç—Ä.
–ù–ï –º–µ–Ω—è–π —Å–º—ã—Å–ª. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
[–í—ã] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
[–°–æ–±–µ—Å–µ–¥–Ω–∏–∫] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç`

	userPrompt := fmt.Sprintf("–£–ª—É—á—à–∏ —ç—Ç—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é:\n\n%s", text)

	reqBody := map[string]interface{}{
		"model": ollamaModel,
		"messages": []map[string]string{
			{"role": "system", "content": systemPrompt},
			{"role": "user", "content": userPrompt},
		},
		"stream":  false,
		"options": map[string]interface{}{"temperature": 0.1, "num_predict": 8192},
	}

	response, err := s.callOllama(ollamaUrl, reqBody)
	if err != nil {
		return nil, err
	}

	return s.parseImprovedDialogue(response, dialogue), nil
}

func (s *LLMService) parseImprovedDialogue(improvedText string, originalDialogue []session.TranscriptSegment) []session.TranscriptSegment {
	lines := strings.Split(improvedText, "\n")
	var improved []session.TranscriptSegment
	lineIdx := 0

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if line == "" {
			continue
		}

		var speaker, text string
		if strings.HasPrefix(line, "[–í—ã]") {
			speaker = "mic"
			text = strings.TrimPrefix(line, "[–í—ã]")
		} else if strings.HasPrefix(line, "[–°–æ–±–µ—Å–µ–¥–Ω–∏–∫]") {
			speaker = "sys"
			text = strings.TrimPrefix(line, "[–°–æ–±–µ—Å–µ–¥–Ω–∏–∫]")
		} else if strings.HasPrefix(line, "–í—ã:") {
			speaker = "mic"
			text = strings.TrimPrefix(line, "–í—ã:")
		} else if strings.HasPrefix(line, "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫:") {
			speaker = "sys"
			text = strings.TrimPrefix(line, "–°–æ–±–µ—Å–µ–¥–Ω–∏–∫:")
		} else {
			continue
		}

		text = strings.TrimSpace(text)
		if text == "" {
			continue
		}

		var start, end int64
		if lineIdx < len(originalDialogue) {
			start = originalDialogue[lineIdx].Start
			end = originalDialogue[lineIdx].End
		}

		improved = append(improved, session.TranscriptSegment{
			Start: start, End: end, Text: text, Speaker: speaker,
		})
		lineIdx++
	}

	if len(improved) == 0 {
		return originalDialogue
	}
	return improved
}

func (s *LLMService) callOllama(baseUrl string, reqBody interface{}) (string, error) {
	jsonBody, _ := json.Marshal(reqBody)
	client := &http.Client{Timeout: 300 * time.Second}
	resp, err := client.Post(baseUrl+"/api/chat", "application/json", bytes.NewBuffer(jsonBody))
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	bodyBytes, _ := io.ReadAll(resp.Body)
	var result struct {
		Message struct {
			Content string `json:"content"`
		} `json:"message"`
		Error string `json:"error"`
	}
	json.Unmarshal(bodyBytes, &result)

	if result.Error != "" {
		return "", fmt.Errorf("Ollama error: %s", result.Error)
	}
	return strings.TrimSpace(result.Message.Content), nil
}

// OllamaModel represents a model from Ollama API
type OllamaModel struct {
	Name       string `json:"name"`
	Size       int64  `json:"size"`
	ModifiedAt string `json:"modified_at"`
	Digest     string `json:"digest"`
	Details    struct {
		Format            string   `json:"format"`
		Family            string   `json:"family"`
		Families          []string `json:"families"`
		ParameterSize     string   `json:"parameter_size"`
		QuantizationLevel string   `json:"quantization_level"`
	} `json:"details"`
}

// GetOllamaModels gets models list from Ollama
func (s *LLMService) GetOllamaModels(baseUrl string) ([]OllamaModel, error) {
	client := &http.Client{Timeout: 5 * time.Second}
	resp, err := client.Get(baseUrl + "/api/tags")
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("Ollama API returned status: %d", resp.StatusCode)
	}

	var result struct {
		Models []OllamaModel `json:"models"`
	}

	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	if err := json.Unmarshal(bodyBytes, &result); err != nil {
		return nil, err
	}

	return result.Models, nil
}
