# Архитектура: Замена Python-зависимости в AIWisper

**Дата:** 2025-12-02
**Статус:** Completed
**Архитектор:** @architect
**На основе анализа:** Текущая архитектура AIWisper

---

## Резюме

Анализ вариантов замены Python-зависимости (faster-whisper, mlx-whisper) на самодостаточное решение для macOS приложения AIWisper.

**Рекомендуемый вариант:** Конвертация всех моделей в GGML формат с использованием whisper.cpp + Metal GPU.

---

## ADR (Architecture Decision Record)

### Контекст проблемы

Текущая архитектура AIWisper использует:
- **Go backend** с whisper.cpp binding для GGML моделей (работает с Metal GPU)
- **Python** для faster-whisper (CTranslate2) и mlx-whisper моделей
- Требуется поддержка русской модели `antony66/whisper-large-v3-russian` (HuggingFace формат)

Проблемы текущего подхода:
1. Зависимость от Python runtime на машине пользователя
2. Сложность установки faster-whisper (pip, зависимости)
3. Два разных движка распознавания
4. Увеличение размера при bundling Python

### Анализируемые варианты

| # | Вариант | Описание |
|---|---------|----------|
| 1 | GGML конвертация | Конвертировать все модели в GGML формат |
| 2 | Python Bundle | Встроить Python runtime в приложение |
| 3 | whisper.cpp only | Использовать whisper.cpp для всех моделей |
| 4 | CoreML | Конвертировать модели в Apple CoreML формат |
| 5 | MLX через cgo | Интегрировать MLX C++ API через cgo |

---

## Детальный анализ вариантов

### Вариант 1: Конвертация в GGML формат ⭐ РЕКОМЕНДУЕМЫЙ

**Описание:**
Конвертировать все HuggingFace модели (включая antony66/whisper-large-v3-russian) в GGML формат и использовать единый whisper.cpp движок.

**Процесс конвертации:**
```bash
# 1. Клонировать репозитории
git clone https://github.com/openai/whisper
git clone https://github.com/ggml-org/whisper.cpp

# 2. Скачать модель
cd whisper.cpp/models
git clone https://huggingface.co/antony66/whisper-large-v3-russian

# 3. Конвертировать в GGML
python3 ./convert-h5-to-ggml.py ./whisper-large-v3-russian/ ../../whisper .

# 4. Опционально квантизировать для уменьшения размера
./quantize ggml-model.bin ggml-model-q5_0.bin q5_0
```

**Оценка:**

| Критерий | Оценка | Комментарий |
|----------|--------|-------------|
| Сложность реализации | 2/5 | Готовые скрипты конвертации |
| Качество распознавания RU | 5/5 | Те же веса модели, без потерь |
| Производительность | 4/5 | Metal GPU ускорение |
| Размер приложения | 5/5 | Минимальный (~50MB без моделей) |
| Поддерживаемость | 5/5 | Единый движок, активное сообщество |

**Плюсы:**
- ✅ whisper.cpp уже интегрирован и работает с Metal GPU
- ✅ Единый движок для всех моделей
- ✅ Минимальный размер приложения
- ✅ Нет зависимости от Python
- ✅ Готовые инструменты конвертации
- ✅ Активное сообщество и поддержка

**Минусы:**
- ⚠️ Нужно конвертировать модели (одноразовая операция)
- ⚠️ Возможна небольшая потеря качества при квантизации (q5_0)

**Доступные русские модели в GGML:**
- `ggml-large-v3.bin` - базовая модель с поддержкой русского
- `ggml-large-v3-turbo.bin` - быстрая версия
- Конвертированная `antony66/whisper-large-v3-russian`

---

### Вариант 2: Встроить Python runtime

**Описание:**
Использовать PyInstaller или py2app для создания standalone Python bundle с faster-whisper.

**Оценка:**

| Критерий | Оценка | Комментарий |
|----------|--------|-------------|
| Сложность реализации | 3/5 | Настройка сборки, подпись кода |
| Качество распознавания RU | 5/5 | Без изменений |
| Производительность | 2/5 | CPU only (faster-whisper не поддерживает Metal) |
| Размер приложения | 2/5 | +150-200MB (Python + зависимости) |
| Поддерживаемость | 2/5 | Сложность обновления зависимостей |

**Плюсы:**
- ✅ Минимальные изменения кода
- ✅ Работает с любыми Python моделями

**Минусы:**
- ❌ Значительное увеличение размера (+150-200MB)
- ❌ Сложность обновления Python зависимостей
- ❌ Проблемы с подписью кода на macOS (notarization)
- ❌ Медленнее (CPU only для faster-whisper на Apple Silicon)
- ❌ Два разных движка распознавания

**Структура bundle:**
```
AIWisper.app/
├── Contents/
│   ├── MacOS/
│   │   ├── aiwisper-backend
│   │   └── python-runtime/
│   │       ├── python3
│   │       ├── faster_whisper/
│   │       └── dependencies/
│   └── Resources/
│       └── models/
```

---

### Вариант 3: whisper.cpp для всех моделей

**Описание:**
Это по сути вариант 1 - whisper.cpp не поддерживает CTranslate2 формат напрямую, требуется конвертация в GGML.

**Вывод:** Объединён с вариантом 1.

---

### Вариант 4: CoreML модели

**Описание:**
Конвертировать модели в Apple CoreML формат для максимальной производительности на Apple Neural Engine (ANE).

**Процесс конвертации:**
```bash
# 1. Установить зависимости
pip install coremltools openai-whisper

# 2. Конвертировать модель
cd whisper.cpp/models
python3 ./convert-h5-to-coreml.py --model large-v3 --encoder-only

# 3. Скомпилировать для устройства
xcrun coremlc compile ggml-large-v3-encoder.mlpackage ./
```

**Оценка:**

| Критерий | Оценка | Комментарий |
|----------|--------|-------------|
| Сложность реализации | 3/5 | Конвертация + интеграция |
| Качество распознавания RU | 5/5 | Те же веса модели |
| Производительность | 5/5 | 3-6x ускорение на ANE |
| Размер приложения | 4/5 | +100MB (CoreML модели больше) |
| Поддерживаемость | 4/5 | Apple нативный формат |

**Плюсы:**
- ✅ Максимальная производительность на Apple Silicon (3-6x ускорение encoder)
- ✅ Нативная интеграция с Apple Neural Engine
- ✅ whisper.cpp поддерживает CoreML (флаг `-DWHISPER_COREML=1`)
- ✅ Оптимизация энергопотребления

**Минусы:**
- ⚠️ Долгая конвертация (20+ минут на модель)
- ⚠️ Первый запуск медленный (компиляция CoreML)
- ⚠️ Больший размер моделей
- ⚠️ Сложнее обновлять модели
- ⚠️ Только encoder на ANE, decoder на CPU/GPU

**Benchmark (из whisper.cpp discussions):**
```
Модель: whisper-small
Encoder time per run:
- whisper.cpp CPU (4 threads): 1030ms
- CoreML (Apple Neural Engine): 174ms
Ускорение: 6x
```

---

### Вариант 5: MLX через cgo

**Описание:**
Интегрировать Apple MLX framework через C API (mlx-c) и cgo.

**Архитектура:**
```
Go Backend
    ↓ cgo
MLX C API (mlx-c)
    ↓
MLX C++ Core
    ↓
Apple GPU (Metal)
```

**Оценка:**

| Критерий | Оценка | Комментарий |
|----------|--------|-------------|
| Сложность реализации | 5/5 | Сложная интеграция C++ с Go |
| Качество распознавания RU | 5/5 | Без изменений |
| Производительность | 4/5 | GPU ускорение |
| Размер приложения | 3/5 | +50-100MB (MLX библиотеки) |
| Поддерживаемость | 2/5 | Сложная зависимость |

**Плюсы:**
- ✅ Нативное GPU ускорение через Metal
- ✅ Есть mlx-c (C API для MLX)
- ✅ Поддержка HuggingFace моделей напрямую

**Минусы:**
- ❌ Очень сложная интеграция C++ библиотеки с Go через cgo
- ❌ Нужно компилировать MLX как статическую библиотеку
- ❌ Зависимость от Apple-специфичных библиотек
- ❌ Сложность отладки и поддержки
- ❌ Нет готовых примеров интеграции с Go

**Пример cgo интеграции:**
```go
/*
#cgo CXXFLAGS: -std=c++17 -I/path/to/mlx-c/include
#cgo LDFLAGS: -L/path/to/mlx-c/lib -lmlx -lmlx_c
#cgo LDFLAGS: -framework Metal -framework Foundation
#include "mlx/c/mlx.h"
*/
import "C"
```

---

## Сравнительная таблица

| Критерий | GGML ⭐ | Python Bundle | CoreML | MLX cgo |
|----------|---------|---------------|--------|---------|
| **Сложность** | 2/5 | 3/5 | 3/5 | 5/5 |
| **Качество RU** | 5/5 | 5/5 | 5/5 | 5/5 |
| **Производительность** | 4/5 | 2/5 | 5/5 | 4/5 |
| **Размер приложения** | 5/5 | 2/5 | 4/5 | 3/5 |
| **Поддерживаемость** | 5/5 | 2/5 | 4/5 | 2/5 |
| **Итого** | **21/25** | 14/25 | 21/25 | 19/25 |

---

## Рекомендация

### Основной вариант: GGML конвертация (Вариант 1)

**Обоснование:**
1. **Минимальная сложность** - готовые инструменты конвертации
2. **Отличная производительность** - Metal GPU ускорение уже работает
3. **Минимальный размер** - только Go бинарник + модели
4. **Высокая поддерживаемость** - единый движок, активное сообщество
5. **Без Python зависимости** - полностью самодостаточное приложение

### Опциональное улучшение: CoreML (Вариант 4)

Если требуется максимальная производительность:
1. Добавить поддержку CoreML в сборку whisper.cpp
2. Конвертировать модели в CoreML формат
3. Использовать ANE для encoder (3-6x ускорение)

---

## План реализации

### Фаза 1: Конвертация моделей (1-2 дня)

```bash
# 1. Подготовка окружения
cd /Users/askid/Projects/AIWisper
git clone https://github.com/openai/whisper ../whisper-original
git clone https://github.com/ggml-org/whisper.cpp ../whisper.cpp-tools

# 2. Скачать русскую модель
cd ../whisper.cpp-tools/models
pip install huggingface_hub
python3 -c "from huggingface_hub import snapshot_download; snapshot_download('antony66/whisper-large-v3-russian', local_dir='./whisper-large-v3-russian')"

# 3. Конвертировать в GGML
python3 ./convert-h5-to-ggml.py ./whisper-large-v3-russian/ ../../whisper-original .
mv ggml-model.bin ggml-large-v3-russian.bin

# 4. Опционально квантизировать
cd ..
cmake -B build
cmake --build build --target quantize
./build/bin/quantize models/ggml-large-v3-russian.bin models/ggml-large-v3-russian-q5_0.bin q5_0
```

### Фаза 2: Обновление кода (1 день)

**Файлы для удаления:**
- `backend/faster_whisper_cli.py`
- `backend/faster_whisper_server.py`

**Файлы для обновления:**

1. `backend/models/registry.go`:
```go
// Удалить faster-whisper модели, добавить GGML русскую модель
{
    ID:          "ggml-large-v3-russian",
    Name:        "Large V3 Russian",
    Type:        ModelTypeGGML,
    Size:        "3.0 GB",
    SizeBytes:   3_100_000_000,
    Description: "Лучшее качество для русского языка (GGML)",
    Languages:   []string{"ru"},
    Speed:       "~2x",
    Recommended: true,
    DownloadURL: "https://huggingface.co/AIWisper/ggml-whisper-russian/resolve/main/ggml-large-v3-russian.bin",
},
```

2. `backend/ai/whisper.go`:
```go
// Удалить:
// - FasterWhisperServer struct и методы
// - transcribeFasterWhisper функцию
// - ensureFasterWhisperInstalled функцию
// - isFasterModel функцию (всегда возвращать false)

// Упростить Engine:
type Engine struct {
    model     whisper.Model
    modelPath string
    language  string
    mu        sync.Mutex
}
```

### Фаза 3: Тестирование (1 день)

1. Тестирование качества на русском аудио
2. Сравнение с faster-whisper результатами
3. Benchmark производительности
4. Тестирование на разных моделях M1/M2/M3/M4

### Фаза 4: Опционально CoreML (2-3 дня)

```bash
# 1. Пересобрать whisper.cpp с CoreML
cd whisper.cpp
cmake -B build -DWHISPER_COREML=1
cmake --build build

# 2. Конвертировать модель в CoreML
cd models
python3 ./convert-h5-to-coreml.py --model large-v3 --encoder-only

# 3. Скомпилировать CoreML модель
xcrun coremlc compile ggml-large-v3-encoder.mlpackage ./
```

---

## Изменения в архитектуре

### До (текущая архитектура)

```
┌─────────────────────────────────────────────────────────────┐
│                      Go Backend                              │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                   Whisper Engine                         ││
│  │  ┌─────────────────┐  ┌─────────────────────────────┐   ││
│  │  │ whisper.cpp     │  │ Python Server               │   ││
│  │  │ (GGML + Metal)  │  │ (faster-whisper/mlx-whisper)│   ││
│  │  └─────────────────┘  └─────────────────────────────┘   ││
│  └─────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────┘
                              ↓
              ┌───────────────┴───────────────┐
              │                               │
        GGML модели                    Python runtime
        (Metal GPU)                    (CPU only)
```

### После (рекомендуемая архитектура)

```
┌─────────────────────────────────────────────────────────────┐
│                      Go Backend                              │
│  ┌─────────────────────────────────────────────────────────┐│
│  │                   Whisper Engine                         ││
│  │  ┌─────────────────────────────────────────────────────┐││
│  │  │              whisper.cpp (GGML + Metal)             │││
│  │  │         Опционально: + CoreML (ANE)                 │││
│  │  └─────────────────────────────────────────────────────┘││
│  └─────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────┘
                              ↓
                    Все модели в GGML формате
                    (Metal GPU / ANE)
```

---

## Архитектурные риски

| Риск | Вероятность | Влияние | Митигация |
|------|-------------|---------|-----------|
| Потеря качества при конвертации | Low | Medium | Тестирование на русском датасете, сравнение WER |
| Отсутствие готовой GGML версии | Medium | Low | Конвертация через whisper.cpp скрипты |
| Проблемы с квантизацией | Low | Low | Использовать fp16 вместо q5_0 если нужно |
| Несовместимость модели | Low | High | Проверить совместимость архитектуры модели |

---

## Нефункциональные требования

### Производительность (после миграции)

| Метрика | Текущее (faster-whisper CPU) | Целевое (GGML Metal) | С CoreML |
|---------|------------------------------|----------------------|----------|
| Транскрипция 15s (large) | ~8-10s | ~4-5s | ~2-3s |
| Использование GPU | 0% | 80-100% | 100% (ANE) |
| Использование CPU | 100% | 20-30% | 10-20% |

### Размер приложения

| Компонент | Текущий | После миграции |
|-----------|---------|----------------|
| Go backend | ~50MB | ~50MB |
| Python runtime | ~150MB | 0 |
| Модели | ~3GB | ~3GB |
| **Итого** | ~3.2GB | ~3.05GB |

---

## Рекомендации для реализации

### Для @planner

**Задачи для декомпозиции:**
1. Конвертация antony66/whisper-large-v3-russian в GGML
2. Обновление registry.go
3. Рефакторинг whisper.go (удаление Python кода)
4. Тестирование качества распознавания
5. Опционально: интеграция CoreML

### Для @coder

**Ключевые изменения:**
1. Удалить `faster_whisper_cli.py`, `faster_whisper_server.py`
2. Упростить `Engine` struct в `whisper.go`
3. Удалить `FasterWhisperServer` и связанный код
4. Обновить `registry.go` - только GGML модели
5. Обновить `isFasterModel()` - всегда возвращать false или удалить

### Для @tester

**Тестовые сценарии:**
1. Качество распознавания русского языка (сравнение с faster-whisper)
2. Производительность на M1/M2/M3/M4
3. Использование GPU (Metal)
4. Размер приложения
5. Время загрузки модели

---

## Заключение

Рекомендуется **Вариант 1 (GGML конвертация)** как основное решение:

- **Минимальная сложность** реализации
- **Отличная производительность** на Metal GPU
- **Полная независимость** от Python
- **Единый движок** для всех моделей
- **Активная поддержка** сообщества whisper.cpp

Опционально можно добавить **CoreML поддержку** для максимальной производительности на Apple Neural Engine (3-6x ускорение encoder).
